# ğŸ§® Level 2: Classical Machine Learning (From Scratch)

<div align="center">
  <h2>âš™ï¸ Core Algorithms Without High-Level Libraries</h2>
  <p><em>First-principles understanding of fundamental ML techniques</em></p>
</div>

---

## ğŸ“‹ **Navigation**
- **[ğŸ  Wiki Home](Home.md)** - Repository overview
- **[ğŸ—ºï¸ Learning Roadmap](Roadmap.md)** - Study progression
- **[ğŸ“– Research Notes](../research-notes/LEVEL_2_RESEARCH_NOTES.md)** - Technical insights
- **[ğŸ’» Code Projects](../Projects2026/Level2/)** - From-scratch implementations

---

## ğŸ¯ **Level Overview**

**Level 2** implements core machine learning algorithms **from scratch** using only NumPy and basic libraries. This approach reveals the **fundamental assumptions** and **inherent trade-offs** underlying each algorithm, providing deep insight into bias-variance decomposition, model capacity, and geometric interpretations of learning.

### ğŸ—ï¸ **Core Focus Areas**
- **Linear Models** - Regression and classification from first principles
- **Decision Trees** - Information-theoretic splitting and ensemble methods
- **Support Vector Machines** - Maximum margin classification
- **Unsupervised Learning** - Clustering and dimensionality reduction

---

## ğŸ“š **Algorithm Implementations**

### ğŸ“ˆ **Linear & Logistic Regression**

#### **Key Concepts**
- **Maximum Likelihood Estimation** - Parameter learning from data
- **Gradient Descent** - Optimization for parameter fitting
- **Regularization** - L1/L2 penalties for complexity control
- **Probabilistic Interpretation** - Logistic as sigmoid + cross-entropy

#### **From-Scratch Implementation**
- **Matrix operations** for efficient computation
- **Numerical stability** considerations
- **Convergence monitoring** and early stopping

#### **Resources**
- **[ğŸ“– Implementation Details](../research-notes/LEVEL_2_RESEARCH_NOTES.md)**
- **[ğŸ’» Linear Regression](../Projects2026/Level2/linear-models-ml/linear_regression.py)**
- **[ğŸ“Š Regularization Comparison](../Projects2026/Level2/linear-models-ml/regularization_comparison.py)**

---

### ğŸŒ³ **Decision Trees & Ensemble Methods**

#### **Key Concepts**
- **Information Gain** - Entropy reduction for splitting
- **Gini Impurity** - Alternative splitting criteria
- **Random Forests** - Bootstrap aggregation and feature subsampling
- **Gradient Boosting** - Sequential error correction

#### **From-Scratch Implementation**
- **Recursive tree building** with stopping criteria
- **Ensemble aggregation** strategies
- **Feature importance** calculation

#### **Resources**
- **[ğŸ“– Tree Algorithms Research](../research-notes/LEVEL_2_RESEARCH_NOTES.md#key-mathematical--algorithmic-insights)**
- **[ğŸ’» Decision Tree](../Projects2026/Level2/decision-trees-ensemble/decision_tree.py)**
- **[ğŸ¯ Random Forest](../Projects2026/Level2/decision-trees-ensemble/random_forest.py)**

---

### ğŸ¯ **Support Vector Machines**

#### **Key Concepts**
- **Maximum Margin Classification** - Optimal hyperplane finding
- **Kernel Methods** - Non-linear classification through feature mapping
- **Soft Margins** - Handling non-separable data
- **Dual Formulation** - Computational efficiency

#### **From-Scratch Implementation**
- **Quadratic programming** for optimal hyperplane
- **Kernel functions** (linear, polynomial, RBF)
- **SMO algorithm** for large-scale optimization

#### **Resources**
- **[ğŸ“– SVM Theory](../research-notes/LEVEL_2_RESEARCH_NOTES.md#common-failure-modes-observed)**
- **[ğŸ’» SVM Implementation](../Projects2026/Level2/svm-from-scratch/svm_linear.py)**
- **[ğŸ”„ Kernel Methods](../Projects2026/Level2/svm-from-scratch/svm_kernel.py)**

---

### ğŸ¨ **Unsupervised Learning**

#### **Key Concepts**
- **K-Means Clustering** - Centroid-based partitioning
- **DBSCAN** - Density-based clustering
- **Dimensionality Reduction** - PCA and t-SNE
- **Evaluation Metrics** - Silhouette scores, purity measures

#### **From-Scratch Implementation**
- **Distance metrics** and similarity measures
- **Expectation-maximization** for probabilistic clustering
- **Manifold learning** techniques

#### **Resources**
- **[ğŸ“– Unsupervised Methods](../research-notes/LEVEL_2_RESEARCH_NOTES.md#empirical-observations)**
- **[ğŸ’» K-Means Algorithm](../Projects2026/Level2/unsupervised-learning/kmeans/kmeans_from_scratch.py)**
- **[ğŸ—ºï¸ Dimensionality Reduction](../Projects2026/Level2/unsupervised-learning/dimensionality_reduction/)**

---

## ğŸ”¬ **Research Insights**

### ğŸ’¡ **Algorithmic Deep Dives**
- **Bias-Variance Tradeoff** - Model complexity vs generalization
- **Geometric Interpretation** - Decision boundaries as mathematical objects
- **Computational Complexity** - Training vs prediction efficiency
- **Robustness Analysis** - Sensitivity to hyperparameters and data

### âš ï¸ **Failure Modes & Debugging**
- **Overfitting** - High variance on unseen data
- **Underfitting** - High bias, inability to capture patterns
- **Numerical Issues** - Ill-conditioned optimization
- **Hyperparameter Sensitivity** - Critical parameter tuning

### ğŸ¤” **Design Decisions**
- **Model Selection** - When to use trees vs linear models vs SVMs
- **Feature Engineering** - Preprocessing impact on algorithm performance
- **Scalability Considerations** - Computational requirements

---

## ğŸ§­ **Learning Progression**

### ğŸ“š **Building on Level 1**
- **Optimization Theory** â†’ Gradient descent in practice
- **Linear Algebra** â†’ Matrix operations for efficiency
- **Probability** â†’ Maximum likelihood parameter estimation
- **Information Theory** â†’ Decision tree splitting criteria

### ğŸ¯ **Level 2 Mastery Goals**
- **Implement algorithms** from mathematical definitions
- **Diagnose model behavior** using theoretical understanding
- **Compare approaches** based on problem characteristics
- **Select appropriate methods** for specific applications

### ğŸš€ **Bridge to Advanced Topics**
- **Neural Networks** - Non-linear extensions of linear models
- **Deep Learning** - Hierarchical feature learning
- **Probabilistic Methods** - Uncertainty quantification

---

## ğŸ“– **Implementation Details**

### ğŸ’» **Code Structure**
Each algorithm includes:
- **Core implementation** with detailed comments
- **Visualization utilities** for understanding
- **Comparison scripts** against scikit-learn
- **Performance analysis** tools

### ğŸ§ª **Empirical Validation**
- **Convergence testing** - Optimization behavior
- **Accuracy comparison** - Against established implementations
- **Robustness analysis** - Parameter sensitivity
- **Computational benchmarking** - Speed and memory usage

### ğŸ”§ **Extensibility**
- **Modular design** - Easy to modify and extend
- **Hyperparameter interfaces** - Systematic tuning
- **Custom loss functions** - Algorithm adaptation

---

## ğŸ“‹ **Complete Project Catalog**

### ğŸ“ˆ **Linear Models**
```
linear-models-ml/
â”œâ”€â”€ linear_regression.py          # OLS implementation
â”œâ”€â”€ logistic_regression.py        # Sigmoid classification
â”œâ”€â”€ regularization_comparison.py  # L1/L2 analysis
â””â”€â”€ bias_variance.py              # Decomposition study
```

### ğŸŒ³ **Decision Trees & Ensembles**
```
decision-trees-ensemble/
â”œâ”€â”€ decision_tree.py              # ID3/CART algorithm
â”œâ”€â”€ random_forest.py              # Bootstrap aggregation
â”œâ”€â”€ feature_importance.py         # Variable significance
â””â”€â”€ split_criteria.py             # Information gain/Gini
```

### ğŸ¯ **Support Vector Machines**
```
svm-from-scratch/
â”œâ”€â”€ svm_linear.py                 # Hard/soft margins
â”œâ”€â”€ svm_kernel.py                 # Kernel trick
â”œâ”€â”€ kernel_visualization.py       # Decision boundaries
â””â”€â”€ nonlinear_classification.py   # RBF applications
```

### ğŸ¨ **Unsupervised Learning**
```
unsupervised-learning/
â”œâ”€â”€ kmeans/                       # Centroid clustering
â”œâ”€â”€ dbscan/                       # Density clustering
â”œâ”€â”€ dimensionality_reduction/     # PCA, t-SNE, UMAP
â””â”€â”€ evaluation/                   # Clustering metrics
```

---

## ğŸ”— **Quick Navigation**

### ğŸ“š **Level Progression**
- **[â† Level 1](Level-1-Mathematical-Foundations.md)** - Mathematical foundations
- **[Level 3 â†’](Level-3-Neural-Networks.md)** - Neural architectures
- **[Level 4 â†’](Level-4-Probabilistic-ML.md)** - Advanced probabilistic methods

### ğŸ“– **Research & Code**
- **[ğŸ“– Full Research Notes](../research-notes/LEVEL_2_RESEARCH_NOTES.md)**
- **[ğŸ’» All Level 2 Projects](../Projects2026/Level2/)**
- **[ğŸ§ª Comparison Scripts](../Projects2026/Level2/linear-models-ml/utils.py)**

### ğŸ  **Repository Navigation**
- **[ğŸ  Wiki Home](Home.md)** - Complete overview
- **[ğŸ—ºï¸ Roadmap](Roadmap.md)** - Learning progression

---

<div align="center">
  <strong>ğŸš€ **Mastered classical ML?** Advance to [Level 3: Neural Networks & Deep Learning](Level-3-Neural-Networks.md)</strong>
</div>