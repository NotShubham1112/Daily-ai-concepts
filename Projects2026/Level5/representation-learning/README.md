# Representation Learning â€” Research-Oriented Topics (Level 5)

This module explores how models learn meaningful latent representations of data, focusing on unsupervised and generative approaches.

## ðŸ“Œ Module Scope

- **Autoencoders vs VAEs**: Implementing the core logic of standard Autoencoders and the probabilistic bottleneck of Variational Autoencoders.
- **Contrastive Learning Logic**: Concepts behind learning by comparing similar and dissimilar pairs.
- **Latent Space Visualization**: Understanding how data is mapped into lower-dimensional manifolds.

---

## ðŸ“‚ Repository Structure

```text
representation-learning/
â”‚
â”œâ”€â”€ autoencoder.py             # Simple Autoencoder implementation
â”œâ”€â”€ vae_conceptual.py          # Variational Autoencoder logic (KL + Recon)
â”œâ”€â”€ latent_space_vis.py        # Visualizing the 2D bottleneck
â”œâ”€â”€ utils.py                   # Data scaling and helper functions
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
