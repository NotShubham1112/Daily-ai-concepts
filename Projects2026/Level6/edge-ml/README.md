# Edge & Resource-Constrained ML â€” ML Systems & Deployment (Level 6)

This module explores techniques for optimizing machine learning models to run efficiently on low-power devices like mobile phones, IoT sensors, and Raspberry Pis.

## ðŸ“Œ Module Scope

- **Model Quantization**: Implementing 8-bit integer quantization to reduce model size by 4x.
- **Weight Pruning**: Removing redundant parameters based on magnitude to speed up inference.
- **Knowledge Distillation**: Training a small "student" model to mimic a larger "teacher" model.

---

## ðŸ“‚ Repository Structure

```text
edge-ml/
â”‚
â”œâ”€â”€ quantization.py            # Weight quantization logic (Int8)
â”œâ”€â”€ pruning.py                 # Structural and magnitude pruning
â”œâ”€â”€ distillation_logic.py      # Student-Teacher loss implementation
â”œâ”€â”€ utils.py                   # Model size/latency benchmarks
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
