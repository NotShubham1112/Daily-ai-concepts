# Attention & Transformers â€” Deep Learning Foundations (Level 3)

This module implements **Attention Mechanisms and Transformers completely from scratch**, focusing on the mathematical and algorithmic foundations of modern sequence modeling.

All components â€” scaled dot-product attention, multi-head attention, and the transformer block â€” are implemented using **NumPy only**, without relying on deep learning frameworks such as PyTorch or TensorFlow.

This module serves as the **bridge between sequence models (RNNs) and the modern Transformer era**.

---

## ðŸ“Œ Module Scope

This module covers:

- Self-Attention Mechanism (Scaled Dot-Product)
- Multi-Head Attention
- Positional Encodings
- Feed-Forward Networks
- Mini-Transformer Architecture
- Attention Weight Visualization

---

## ðŸ“‚ Repository Structure

```text
attention-and-transformers/
â”‚
â”œâ”€â”€ attention.py               # Self-Attention & Multi-Head Attention
â”œâ”€â”€ transformer.py             # Mini-Transformer Implementation
â”œâ”€â”€ positional_encoding.py     # Positional Encoding Logic
â”œâ”€â”€ heatmap_visualization.py   # Attention Heatmap Visualization
â”œâ”€â”€ utils.py                   # Helper utilities (Softmax, etc.)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
