# Gradient Descent

## 1. Problem This Solves
How do we minimize loss efficiently?

## 2. Intuition
Move parameters slightly in the direction of lower loss.

## 3. Algorithm
θ = θ − α∇L

## 4. Why This Works
Local slope guides global improvement (sometimes).

## 5. Practical Issues
- Learning rate
- Local minima
- Saddle points

## 6. Next Concepts
- Stochastic gradient descent
